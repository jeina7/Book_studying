{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4. word2vec 속도 개선"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\# 4.1 word2vec 개선 1 : Embedding Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 앞서 구현했던 `MatMul` layer의 방식으로는 너무 많은 계산을 하게 됨.\n",
    "\n",
    "\n",
    "- 사실 one-hot vector인 input 벡터와 `W_in` 가중치의 내적은 one-hot vector의 특성 상 그냥 `W_in` 가중치의 특정 행을 추출하는 것\n",
    "\n",
    "\n",
    "- 따라서 내적 곱을 하는 layer를 이용하지 않고 단순히 slicing으로 `W_in` 의 특정 행을 추출하는 `Embedding Layer`를 만들어 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.idx = None\n",
    "        \n",
    "    def forward(self, idx):\n",
    "        W, = self.params\n",
    "        self.idx = idx\n",
    "        out = W[idx]\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dW, = self.grads\n",
    "        \n",
    "        # dW의 모든 값을 0으로 덮어씌우기\n",
    "        dW[...] = 0\n",
    "        \n",
    "        # idx에 해당하는 행에 dout값을 모두 더해주기\n",
    "        np.add.at(dW, self.idx, dout)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 예제 text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Thomas Jefferson once said I'm a great believer in luck, and I find the harder I work, \\\n",
    "the more I have of it. What, though, is luck? Webster's dictionary suggests that luck is the \\\n",
    "events or circumstances that operate for or against an individual. In truth, luck has nothing \\\n",
    "to do with something operating for or against you. Luck is not a matter of chance. \\\n",
    "It is a matter of being open to new experiences, perseverance, hard work, and positive thinking. \\\n",
    "When seventeen year old Steven Spielberg spent some time with his cousin in the summer of 1965, \\\n",
    "they toured Universal pictures. The tram stopped at none of the sound stages. \\\n",
    "Spielberg snuck off on a bathroom break to watch a bit of the real action. \\\n",
    "When he encountered an unfamiliar face who demanded to know what he was doing, \\\n",
    "he told him his story. The man turned out to be the head of the editorial department. \\\n",
    "Spielberg got a pass to the lot for the very next day and showed a very impressed \\\n",
    "Chuck Silvers four of his eight millimeter films. This was the foot in the door Spielberg \\\n",
    "needed to start squatting on the lot, a decision that led to his first contract with Universal Studios. \\\n",
    "Studies have shown that lucky people tend to be far more open to new experiences. Those who are \\\n",
    "unlucky are creatures of habit, never varying from one day to the next. If you want to be lucky, \\\n",
    "add some variety to your life. Meet new people, go to new places, and increase the possibility of \\\n",
    "those chance opportunities the lucky people always seem to run into.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### preprocess 함수로 말뭉치 corpus 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.util import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11,  4, 12, 13, 14,  4,\n",
       "       15, 13, 16,  4, 17, 18, 19, 20, 21, 22, 23, 10, 24, 25, 26, 27, 28,\n",
       "       29, 10, 23, 13, 30, 31, 32, 29, 33, 34, 31, 35, 36, 37, 20,  9])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "corpus[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### unique한 단어는 165개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_to_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 임시방편으로 W는 165개의 단어별로 3개의 노드를 갖는 (165 x 3) shape로 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2],\n",
       "       [ 3,  4,  5],\n",
       "       [ 6,  7,  8],\n",
       "       [ 9, 10, 11],\n",
       "       [12, 13, 14]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = np.arange(165*3).reshape(165, 3)\n",
    "W[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 추출하고자 하는 단어 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[112, 114]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = ['very', 'day']\n",
    "context_idx = [word_to_id[word] for word in context]\n",
    "context_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[113]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = ['next']\n",
    "target_idx = [word_to_id[word] for word in target]\n",
    "target_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Embedding` class의 forward 메서드를 이용해서 idx에 해당하는 행을 바로 추출 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contexts: [[336 337 338]\n",
      " [342 343 344]]\n",
      "\n",
      "target: [[339 340 341]]\n"
     ]
    }
   ],
   "source": [
    "embed = Embedding(W)\n",
    "\n",
    "context_W = embed.forward(context_idx)\n",
    "target_W = embed.forward(target_idx)\n",
    "\n",
    "print('contexts: ', context_W, '\\n\\n', 'target: ', target_W, sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\# 4.2.4 다중분류에서 이중분류로 : `EmbeddingDot`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `W_out` 쪽에서의 연산 또한 계산량이 너무 크다는 문제가 있다.\n",
    "\n",
    "\n",
    "- 'target 단어가 'say'입니까?' 라고 묻는 기존의 다중 분류 방식 `multi-class classification`에서,    \n",
    "'target 단어가 'say'가 맞습니까?' 라고 묻는 이진 분류 방식 `binary class classification`으로 변경\n",
    "\n",
    "\n",
    "\n",
    "### 연산 과정\n",
    "- target 단어인 say를 idx 형태로 입력받으면 Embedding class로 forward 시켜서 target_W 를 출력\n",
    "\n",
    "\n",
    "- 출력된 target_W 의 열벡터를 입력받는 h벡터 (입력으로 받은 context들로부터 생성된 벡터) 와 dot 연산을 통해 out 값 (scalar값)을 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDot:\n",
    "    def __init__(self, W):\n",
    "        self.embed = Embedding(W)\n",
    "        self.params = self.embed.params\n",
    "        self.grads = self.embed.grads\n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, h, idx):\n",
    "        target_W = self.embed.forward(idx)\n",
    "        out = np.sum(target_W * h, axis=1)\n",
    "        self.cache = (h, target_W)\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        h, target_W = self.cache\n",
    "        dout = dout.reshape(dout.shape[0], 1)\n",
    "        \n",
    "        # dot 연산에 대한 backward\n",
    "        # dout의 값에 각각 서로를 바꿔서 곱해준 값이 gradient값\n",
    "        dtarget_W = dout * h\n",
    "        dh = dout * target_W\n",
    "        # embed 계층 쪽에도 backward 수행\n",
    "        self.embed.backward(dtarget_W)\n",
    "        return dh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `h` : context 들의 context_W를 합쳐서 평균낸 벡터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([339., 340., 341.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h0 = context_W[0] \n",
    "h1 = context_W[1]\n",
    "h = (h0 + h1) / 2\n",
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EmbeddingDot 의 forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([346802.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_dot = EmbeddingDot(W)\n",
    "\n",
    "out = embed_dot.forward(h, target_idx)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EmbeddingDot 의 backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[339, 340, 341]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dout = np.array([1])\n",
    "dh = embed_dot.backward(dout)\n",
    "dh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\# 4.2.5 Negative Sampling의 sampling 기법 : `UnigramSampler`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 다중분류에서 이진분류로 옮겨오면서 '틀린' 경우에 대한 학습을 모든 어휘에 대해 하지 않고 특정 단어들을 샘플링해서 일부분만 학습하는 방법을 취한다.\n",
    "\n",
    "\n",
    "- 예를 들어, context가 `'you'`, `'goodbye'`일 때 정답은 `'say'`인 경우를 생각해보자.\n",
    "\n",
    "\n",
    "- 모델은 'say'를 `label 1`로 **한 번** 학습하고, 그 외의 틀린 단어들을 `label 0`으로 **여러 번** 학습한다. \n",
    "\n",
    "\n",
    "- 이 때 틀린 단어들 (negative samples)에 대해서는 특정 개수를 정해서 **그 횟수만큼만** 학습한다.    \n",
    "(연산량을 줄이기 위해 꼭 필요한 만큼만 학습)\n",
    "\n",
    "\n",
    "- 이 때 sampling 에 쓰이는 작은 트릭은, \n",
    "**많이 쓰이는 단어들에 대해서는 sampling될 확률을 좀 더 크게 주고, \n",
    "잘 안쓰이는 단어에 대해서는 확률을 좀 작게 주는 것.**\n",
    "\n",
    "\n",
    "- 이로써 더 많이 쓰이는 단어들에 대해 더 많이 학습하고 적게 쓰이는 단어들은 적게 학습하는 효과를 기대할 수 있다.    \n",
    "(별로 안쓰이는 단어를 많이 학습할 필요는 없다는 말)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### negative sampling을 위한 `UnigramSampler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnigramSampler:\n",
    "    def __init__(self, corpus, power, sample_size):\n",
    "        self.sample_size = sample_size\n",
    "        self.vocab_size = len(set(corpus))\n",
    "        self.word_p = np.zeros(self.vocab_size)\n",
    "        \n",
    "        # 각 단어가 몇 번씩 나오는지 저장\n",
    "        counts = collections.Counter(corpus)\n",
    "        \n",
    "        # 각 단어의 출현횟수를 확률로 변환하여 word_p에 저장\n",
    "        self.word_p = [counts[i] for i in range(self.vocab_size)]\n",
    "        self.word_p = np.power(self.word_p, power)\n",
    "        self.word_p /= np.sum(self.word_p)\n",
    "        \n",
    "    def get_negative_sample(self, target):\n",
    "        batch_size = target.shape[0]\n",
    "        \n",
    "        negative_sample = np.zeros((batch_size, self.sample_size), dtype=np.int32)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            p = self.word_p.copy()\n",
    "\n",
    "            # target은 뽑히지 않도록 하기 위해 확률을 0으로 설정\n",
    "            target_idx = target[i]\n",
    "            p[target_idx] = 0\n",
    "\n",
    "            p /= p.sum()\n",
    "            negative_sample[i, :] = np.random.choice(self.vocab_size, size=self.sample_size, \\\n",
    "                                                     replace=False, p=p)\n",
    "            \n",
    "        return negative_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 각 단어에 대한 확률값 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.00421884, 0.00421884, 0.00421884, 0.00421884, 0.01193269,\n",
       "       0.00421884, 0.02006832, 0.00421884, 0.00421884, 0.01193269])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_sampler = UnigramSampler(corpus, power=0.75, sample_size=5)\n",
    "\n",
    "print(len(uni_sampler.word_p))\n",
    "uni_sampler.word_p[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 전체 합 1임을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000000000009"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(uni_sampler.word_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### negative_sample 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[159,   2, 128, 158,  50],\n",
       "       [108,  52,  44, 133, 138],\n",
       "       [ 11, 123,  28,  23,  61],\n",
       "       [ 11, 144, 155,   6,  43],\n",
       "       [ 54, 122, 102, 114,  41]], dtype=int32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = np.array([5, 10, 3, 2, 45])\n",
    "\n",
    "negative_sample = uni_sampler.get_negative_sample(target)\n",
    "negative_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 확률이 높은 6이 두 번 나왔고, target값은 제회하고 sampling 된 것을 확인할 수 있다.\n",
    "\n",
    "\n",
    "- 6은 무슨 단어일까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_word[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'a'로, 많이 나오는 단어가 맞다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\# 4.2.7 Negative Sampling 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 먼저 `SigmoidWithLoss` layer 를 구현한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.layer import Sigmoid\n",
    "from common.function import cross_entropy_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = 1 / (1 + np.exp(-x))\n",
    "        \n",
    "        self.loss = cross_entropy_error(np.c_[1 - self.y, self.y], self.t)\n",
    "        \n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        \n",
    "        dx = (self.y - sefl.t) * dout / batch_size\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `NegativeSamplingLoss` 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeSamplingLoss:\n",
    "    def __init__(self, W, corpus, power=0.75, sample_size=5):\n",
    "        self.sample_size = sample_size\n",
    "        self.sampler = UnigramSampler(corpus, power, sample_size)\n",
    "        \n",
    "        # negative sample size에 정답 sample을 위해 1을 더해줌\n",
    "        self.loss_layers = [sigmoidWithLoss() for _ in range(sample_size + 1)]\n",
    "        self.embed_dot_layers = [EmbeddingDot(W) for _ in range(sample_size + 1)]\n",
    "        \n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.embed_dot_layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "            \n",
    "    def forward(self, h, target):\n",
    "        batch_size = target.shape[0]\n",
    "        negative_sample = self.sampler.get_negative_sample(target)\n",
    "        \n",
    "        # Positive sample\n",
    "        score = self.embed_dot_layers[0].forward(h, target)\n",
    "        correct_label = np.ones(batch_size, dtype=np.int32)\n",
    "        loss = self.loss_layers[0].forward(score, correct_label)\n",
    "        \n",
    "        # Negative sample\n",
    "        negative_label = np.zeros(batch_size, dtype=np.int32)\n",
    "        for i in range(self.sample_size):\n",
    "            negative_target = negative_sample[:, i]\n",
    "            score = self.embed_dot_layers[1+i].forward(h, negative_target)\n",
    "            loss += self.loss_layers[1+i].forward(score, negative_label)\n",
    "            \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        dh = 0\n",
    "        for loss_layer, embed_dot_layer in zip(self.loss_layers, self.embed_dot_layers):\n",
    "            dscore = loss_layer.backward(dout)\n",
    "            dh += embed_dot_layer.backward(dscore)\n",
    "            \n",
    "        return dh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
