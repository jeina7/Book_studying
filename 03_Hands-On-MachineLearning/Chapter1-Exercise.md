### \# 핸즈온 머신러닝 ㅤ# Hands-On Machine Learning

# Chapter 1. 한 눈에 보는 머신러닝

Chapter 1 연습문제에 대한 답을 정리하여 담았습니다.  
책에 있는 내용, 또는 추가적으로 공부한 내용들로 구성하였습니다.  
ㅤ

---

#### Keywords

`#머신러닝` `#지도학습` `#비지도학습` `#강화학습` `#분류` `#회귀` `#클러스터링` `#군집`  
`#배치학습` `#미니배치학습` `#모델파라미터` `#하이퍼파라미터` `#오버피팅` `#언더피팅`  
`#훈련데이터` `#검증데이터` `#테스트데이터` `#교차검증`

---

ㅤ

## 1\. 머신러닝을 어떻게 정의할 수 있나요?

책에서 소개하는 머신러닝에 대한 일반적인 두 가지 정의는 다음과 같습니다.

ㅤ

> \[머신러닝\]은 명시적인 프로그래밍 없이 컴퓨터가 학습하는 능력을 갖추게 하는 연구분야다. (Arthur Samuel, 1959)

ㅤ

> 어떤 작업 T에 대한 컴퓨터 프로그램의 성능을 P로 측정했을 때 경험 E로 인해 성능이 향상됐다면, 이 컴퓨터 프로그램은 작업 T와 성능 측정 P에 대해 경험 E로 학습한 것이다. (Tom Mitchell, 1997)

ㅤ  
이 중 한 번에 직관적으로 다가오지 않는 두 번째 정의를 풀어서 설명하자면 다음과 같습니다.  
ㅤ  
1) 프로그램이 `작업 T`를 수행하는 능력을 `성능 P`라고 한다.  
2) 어떤 `경험 E`로 인해서 프로그램의 `성능 P`가 향상되었다.  
3) 이 때, 프로그램은 `작업 T`를 `경험 E`로 학습하였다고 할 수 있다.  
ㅤ  

이 때 `경험 E`는 인간이 직접적으로 코딩을 하거나 명시적인 프로그래밍을 입력받는 것이 아닌, "**데이터**"입니다.  
즉, 머신러닝은 **데이터로부터 성능이 향상되는(학습하는) 시스템을 만드는 것**이라고 할 수 있습니다.

ㅤ

## 2\. 머신러닝이 도움을 줄 수 있는 문제 유형 네 가지는 무엇인가요?

책에서 구분한 문제 유형 네 가지는 다음과 같습니다.

-   **기존의 솔루션으로는 많은 수동 조정과 규칙이 필요한 문제** : 머신러닝 모델이 간단하고 더 빠르게 수행되도록 할 수 있습니다.
-   **전통적인 방식으로는 전혀 해결 방법이 없는 복잡한 문제** : 머신러닝 기법으로 해결 방법을 찾을 수 있습니다.
-   **유동적인 환경에 속해있는 문제** : 머신러닝 시스템은 새로운 데이터에도 실시간으로 적응할 수 있습니다.
-   **복잡한 문제와 대량의 데이터로부터 통찰 얻기** : 문제를 해결하는 것뿐만 아니라 머신러닝 학습 과정으로부터 통찰(insight)을 얻을 수도 있습니다.


즉, 머신러닝을 이용하면 전통적인 방법으로는 **해결이 어려웠던 복잡한 문제들을 해결할 실마리**를 찾을 수 있을뿐만 아니라, 대량의 연산을 처리하는 머신러닝 학습 과정으로부터 **생각지 못했던 정보나 통찰**을 얻을 수도 있습니다.

ㅤ

## 3\. 레이블된 훈련 세트란 무엇인가요?

레이블 된 훈련 세트란, **특징 정보(feature)** 를 담는 training data에 사용자가 원하는 **정답 정보(label)** 함께 담겨있는 데이터셋을 말합니다.  
예를 들어,

-   꽃 종류 분류 문제의 경우
    -   **특징(feature)** : `꽃의 크기`, `잎의 크기` 등
    -   **정답(label)** : `꽃의 종류` (카테고리 데이터)

-   자전거 대여량을 예측하는 회귀 문제의 경우
    -   **특징(feature)정보** : `요일`, `날씨`, `기온` 등
    -   **정답(label)** : `자전거 대여량` (실수형 데이터)  


와 같은 데이터셋이 레이블 된 훈련세트 (labeled train dataset) 입니다.

ㅤ

## 4\. 가장 널리 사용되는 지도 학습 작업 두 가지는 무엇인가요?

지도학습은 훈련 또는 학습 (train) 과정 중에 맞춰야 하는 정답 정보가 함께 제공되는 학습을 말합니다.  
가장 널리 사용되고 또한 가장 보편적인 지도학습 작업의 두 가지는 **`분류 Classification`** 와 **`회귀 Regression`** 입니다.

-   **`분류`** 는 데이터가 **어떤 카테고리에 속하는지**, **카테고리를 예측**하는 문제입니다. 따라서 정답이 될 값이 **이산(descrete)** 값이며 **유한**한 값입니다. 분류 문제의 가장 대표적인 예는 꽃의 종류 예측하기, 타이타닉호에 탑승한 승객의 생존 여부 예측하기 등이 있습니다. 꽃 문제의 경우 정답 카테고리는 `[장미, 튤립, 국화]` 등이 될 수 있으며 타이타닉 문제의 경우 정답 카테고리는 `[1(생존), 0(사망)]`이 될 수 있습니다.
-   반면, **`회귀`** 는 데이터의 **특정 값 자체를 예측**하는 문제입니다. 따라서 정답이 될 값은 **연속(continuous)** 한 값이며 **무한**한 범위에서 존재하는 값입니다. 회귀 문제의 대표적인 예는 자전거 대여량 예측하기, 집값 예측하기 등이 있습니다. 이 경우 두 문제 다 정답은 `[x : x는 양수인 실수]`라는 광범위하면서 연속적인 값을 가질 수 있습니다.

ㅤ

## 5\. 보편적인 비지도 학습 작업 네 가지는 무엇인가요?

비지도학습은 지도학습과 달리 정답 정보가 제공되지 않는 상태에서 학습이 이루어집니다.  
보편적인 비지도 학습 네 가지는 **`군집 Clusturing`**, **`시각화 Visualization`**, **`차원축소 Demensionality Reduction`**, 그리고 **`연관규칙학습 Association Rule Learning`** 입니다.

-   **`군집`** 은 주어진 **데이터를 특정 개수의 그룹 (군집, cluster) 으로 나누는 작업**입니다. 각 데이터 원소가 원래 어떤 그룹인지는 주어지지 않은 상태에서(즉, 정답이 없는 상태에서) 각 데이터들이 모여있는 정도, 각 데이터간의 거리 등의 정보를 활용해서 그룹화하는 알고리즘입니다. k-평균(k-means), DBSCAN, 계층군집분석(HCA) 등의 알고리즘이 있습니다.
-   **`시각화`** 는 **`차원축소`** 와 같은 맥락에서 볼 수 있습니다. 머신러닝에서 활용되는 데이터는 일반적으로 굉장히 높은 차원으로 이루어져 있습니다. 우리는 3차원 이상에 존재하는 데이터는 눈으로 볼 수 없기때문에 **시각화를 하려면 데이터를 높은 차원에서 낮은 차원으로 차원 축소를 해야**합니다. 2차원 또는 3차원으로 축소를 시키면 우리는 데이터를 좌표평면 또는 좌표공간에 시각화해서 볼 수 있습니다.
-   **`차원축소`** 또한 정답정보(label)가 없는 상태에서 데이터를 **N차원에서 낮은 차원으로 축소**시키는 비지도 학습 알고리즘입니다. 차원축소는 시각화 외에도, 불필요하게 데이터의 크기가 너무 클 때 **효과적인 차원축소**를 통한다면 정보는 많이 잃지 않으면서 데이터 크기를 줄여서 연산량을 줄일 수 있습니다. 차원축소는 N차원에 존재하는 데이터, 즉 "특성(feature)이 N가지" 존재하는 데이터에서 상관관계가 높은 특성을 하나로 합쳐 특성 개수를 줄이는 방법이 있고, 이를 **특성추출(Feature Extraction)** 이라고 합니다. 구체적인 차원축소 알고리즘으로는 주성분 분석(PCA), t-SNE 등이 있습니다.
-   **`연관규칙학습`** 은 각 데이터끼리의 연관성을 분석해서 규칙을 찾아 **데이터와 데이터 간의 관계를 수치적으로 예측**하는 알고리즘입니다. 이 알고리즘은 자연어 처리에서 많이 쓰입니다. 예를 들어 문장 생성 또는 문장 요약 등의 작업에서, 단어간의 연관성을 사전에 학습한 후 문장을 만들어 낼 때 "나는 오늘 공부를" 까지의 문장이 있다면 그 뒤의 데이터로 올 수 있는 것을 **연관성이 높은 데이터 순으로** 출력할 수 있습니다. 효과적으로 학습이 되었다면 "한다", "했다", "할 것이다" 등의 문장을 만들어 낼 수 있을 것입니다.

ㅤ

## 6\. 사전 정보가 없는 여러 지형에서 로봇을 걸어가게 하려면 어떤 종류의 머신러닝 알고리즘을 사용할 수 있나요?

사전 정보가 전혀 없는, 즉 데이터가 전혀 주어지지 않는 제로베이스 상태에서 학습을 시작해서 스스로 최적의 알고리즘을 찾아내는 학습을 **`강화학습`** 이라고 합니다. 로봇을 걷게 만드는 작업은 강화학습을 이용해서 이루어질 수 있습니다. 강화학습은 위의 지도학습, 비지도학습 등의 알고리즘과 매우 다른 알고리즘입니다. 유명세를 탔던 알파고도 강화학습 알고리즘을 통해 학습되었습니다.  
강화학습을 일반적으로 설명할 때 **`에이전트 Agent`**, **`환경 Environment`**, **`행동 Action`**, **`보상 Reward`**, **`정책 Policy`** 등의 단어를 이용합니다. 이를 로봇이 걸어가는 상황에 대입해서 간단히 정리하면 다음과 같습니다.

-   **`에이전트`** : `로봇`  
    학습하는 시스템 그 자체를 말합니다.
-   **`환경`** : `여러 지형`  
    로봇이 학습해 나가야 하는 주변 환경을 말합니다. 초기에 로봇은 환경에 대한 정보가 아무것도 없고, 점차 알아나가야 합니다.
-   **`행동`** : `로봇이 취하는 여러 행동`  
    처음에 아무 사전 학습이 되어있지 않을 때는 랜덤으로 매 순간의 행동을 선택할 수 있습니다. 예를 들어 왼쪽 발을 10cm 앞으로 옮기기, 오른쪽 발을 10cm 오른쪽에 딛기 등이 있을 수 있고, 점차 환경을 학습해 나가면서 최적화된 행동들을 선택할 수 있게 됩니다.
-   **`보상`** : `로봇이 한 행동에 대해서 받는 피드백`  
    안전한 지형에 발을 딛었을 경우 보상으로 양(+)의 점수를 주고, 위험한 지형에 발을 딛었을 경우 음(-)의 보상 또는 패널티를 받도록 알고리즘을 짤 수 있습니다. 로봇은 보상에 따라 더 좋은 보상을 받는 행동들이 뭔지 알 수 있고, 그 정보들을 종합하면서 전략을 짤 수 있게 됩니다.
-   **`정책`** : `로봇이 학습하는 전략`  
    로봇은 계속해서 `행동선택 → 보상받기 → 보상에 따라 행동 수정`의 과정을 거치면서 보상이 최대화되는 행동의 일련 과정 (sequence)을 학습하게 될 것입니다. 이렇게 학습된 최상의 전략, 또는 알고리즘을 **정책**이라고 합니다.

ㅤ

## 7\. 고객을 여러 그룹으로 분할하려면 어떤 알고리즘을 사용해야 하나요?

고객을 그룹을 분할할 때에는 두 가지 케이스가 있을 수 있습니다.

-   고객이 나뉘어질 **그룹이 사전에 정의되어 있다면** 지도학습의 **`분류`** 문제로 풀 수 있습니다. 이 경우 학습데이터는 고객의 여러 특징 정보와 함께 그 고객이 어떤 그룹인지 label을 담고 있어야 합니다.
-   고객이 나뉘어질 **그룹을 사전에 알고있지 못하다면** 비지도학습의 **`군집`** 문제로 풀 수 있습니다. 이 경우에는 학습데이터로 고객의 특징 정보만 담긴 데이터를 입력하면 군집 알고리즘을 이용해서 군집화할 수 있을 것입니다.

ㅤ

## 8\. 스팸 감지의 문제는 지도 학습과 비지도 학습 중 어떤 문제로 볼 수 있나요?

특정 이메일이 스팸 메일인지 아닌지를 판별하는 문제는 "스팸인지 아닌지" 라는 label이 주어지므로 **지도학습**의 **분류** 문제입니다.

ㅤ

## 9\. 배치학습 시스템과 온라인 학습 시스템이 무엇인가요?

머신러닝의 학습 시스템은 **입력데이터를 실시간으로 (또는 점진적으로) 학습할 수 있는지의 여부**에 따라 **`배치학습`** 과 **`온라인학습`** 으로 구분할 수 있습니다.

-   `배치학습`은 **주어진 학습데이터를 모두 사용해서 학습**합니다. 그러므로 시간과 자원이 많이 소모되고, 새로운 데이터가 들어오는 것에 대해 대응할 수 없으므로 오프라인에서 수행되어서 `오프라인 학습`이라고도 합니다. 모든 데이터를 이용해서 한 번 학습된 모델에 만약 새로운 데이터가 들어온다면 그 데이터를 포함해서 다시 모든 데이터로 새롭게 모델을 학습시켜야 하기 때문에 다시 시간과 자원을 많이 소모하게 됩니다. 일반적으로 데이터의 크기가 작다면 이 방법이 간단하고 잘 작동하므로 많이 쓰이지만, 데이터 크기가 크다면 배치학습은 비효율적인 방법이 될 수 있습니다.
-   `온라인 학습`은 데이터를 순차적으로 한 개 또는 **미니배치(mini-batch)** 를 이용해서 학습하는 방법이고, 따라서 `미니배치 학습`이라고도 합니다. 배치학습과 달리 한 번에 학습하는 데이터셋이 전체 데이터로부터 쪼개진 작은 데이터셋이기 때문에 적은 연산량과 짧은 시간만으로도 학습이 가능합니다. 또한 새로운 데이터가 들어왔을 경우 앞으로 학습할 미니배치의 대기열에 간단히 추가만 하면 되므로 상황 변화에 대한 대응이 유연합니다. 데이터 크기가 작다면 굳이 미니배치로 또다시 쪼개는 것은 불필요하지만, 데이터 크기가 크다면 미니배치 학습을 이용하는 것이 훨씬 효율적일 것입니다.

ㅤ

## 10\. 외부 메모리 학습이 무엇인가요?

일반적인 머신러닝 작업의 경우 데이터 크기가 매우 클 수 있습니다. 따라서 컴퓨터의 메모리에 들어갈 수 없는 아주 큰 데이터셋을 학습할 때, 방대한 **전체 데이터를 한 번에 모두 메모리에 넣지 않고 점진적으로 넣으면서 학습하는 과정**을 외부 메모리 학습이라고 합니다.  
외부 메모리 학습은 온라인 학습(미니배치 학습) 알고리즘을 이용해서 수행될 수 있습니다.

ㅤ

## 11\. 예측을 하기 위해 유사도 측정에 의존하는 학습 알고리즘은 무엇인가요?

머신러닝 학습 시스템은 **어떻게 일반화되는지**, 즉 **훈련데이터에서 본 적 없는 데이터에 대해서 예측을 어떻게 이루어 내는지**에 대한 방법에 따라 **`사례 기반 학습 instance-based learning`** 과 **`모델 기반 학습 model-based learning`** 으로 구분할 수 있습니다. 일반적으로 현대에서 말하는 머신러닝은, 모두 **모델 기반 학습**입니다.

-   **`사례 기반 학습`** 은 단순히 **훈련 데이터를 기억하는 것**입니다. 그 말은, 새로운 데이터를 보았을 때 그 데이터가 내가 알고 있는 훈련데이터 중 어느 쪽이랑 더 가까운지만으로 새로운 데이터에 대한 예측을 만들어 냅니다. 예를 들어, 스팸메일 분류 문제에서 새로운 메일이 왔을 때 훈련데이터에 있는 일반메일과 스팸메일 각각에 대해 유사도를 측정해서 어떤 카테고리에 더 가까운지를 계산한 후 새로운 메일의 카테고리를 결정합니다.
-   반면 **`모델 기반 학습`** 은 훈련데이터를 기반으로 **모델을 만들어서 예측**합니다. 여기서 모델은 "결정 경계(Decision Boundary)" 등의 **"함수"** 를 말합니다. 즉, 모델은 데이터를 입력받아서 그 데이터에 대한 일련의 계산 과정을 거친 후 그 데이터에 대한 예측값을 출력하는 일종의 "함수"라고 할 수 있습니다. 예를 들어 스팸메일 분류 문제에서 새로운 메일이 왔을 때 그 메일을 입력받아서 일련의 계산과정을 거친 후 그 메일이 스팸메일일 확률값을 출력할 것입니다. 여기서 모델은 머신러닝 알고리즘을 통해 메일을 최적으로 분류할 수 있도록 학습되는 과정을 거칩니다.

따라서 예측을 하기 위해 유사도 측정에 의존하는 학습 알고리즘은 "사례기반 학습" 알고리즘 입니다.

ㅤ

## 12\. 모델 파라미터와 학습 알고리즘의 하이퍼 파라미터 사이에는 어떤 차이가 있나요?

-   모델 파라미터는 모델이 학습을 하면서 점차 최적화되는, 그리고 최적화가 되어야 하는 파라미터입니다. 예를 들어 선형 회귀의 경우 `y_pred = Wx + b`라는 식으로 예측값을 만들어 낼 텐데, 여기에서 모델 파라미터는 `W` 입니다. 모델은 학습 과정을 거치면서 최적의 `y_pred` 값, 즉 `y_true`와 가장 가까운 값을 출력해낼 수 있는 최적의 `W`를 찾아나갈 것입니다.
-   하이퍼 파라미터는 모델이 학습을 하기 위해서 사전에 사람이 직접 입력해주는 파라미터입니다. 이는 모델이 학습하는 과정에서 변하지 않습니다. 예를 들어 학습 횟수에 해당하는 `Epoch 수`, 가중치를 업데이트 할 `학습률(learning rate)`, 또는 선형 규제를 담당하는 `labmda` 값 등이 이에 해당합니다.

ㅤ

## 13\. 모델 기반 학습 알고리즘이 찾는 것은 무엇인가요? 성공을 위해 이 알고리즘이 사용하는 가장 일반적인 전략은 무엇인가요? 예측은 어떻게 만드나요?

-   모델 기반 학습 알고리즘에서 모델은 훈련 과정을 거치면서 새로운 데이터에 대해서도 예측값을 잘 만들어 내기위해, 즉 잘 "일반화"되기 위해 **모델 파라미터값을 최적화 시키는 것**이 목표입니다.

-   이 목표를 달성하기 위해서 모델은 다음과 같은 순서의 전략을 이용합니다.  
    ㅤ  
    1) 데이터를 입력받아서 예측값을 출력합니다.  
    2) 예측값과 실제값을 비교해서 내 예측값이 얼마나 나쁜지를 측정합니다.  
    3) 측정한 값 "손실 함수" 또는 "비용 함수"라고 하며, 이 값을 최소화하는 방향으로 모델 파라미터를 업데이트합니다.  
    ㅤ  

 1~3번의 과정을 반복하면서 모델 파라미터는 점차적으로 손실함수 값을 최소화 할 수 있는 값으로 최적화됩니다.  
 3번에서 **"최소화 하는 방향으로 모델 파라미터를 업데이트"** 하는 방법에는 여러 알고리즘이 있으며, 그 중 가장 대표적인 알고리즘은 **`경사 하강법(Gradient Descent)`** 입니다.

-   예측값은 모델이 학습을 마쳤을 경우 **새로운 데이터를 입력해서 출력하는 값**입니다.


ㅤ

## 14\. 머신러닝의 주요 도전 과제는 무엇인가요?

책에서 소개하는 머신러닝의 주요 도전 과제는 다음과 같습니다.

-   **`부족한 데이터`** : 데이터가 적을 경우 모델은 학습하기가 매우 어렵습니다. 따라서 좋은 모델을 만드려면 무엇보다 **충분한 데이터**를 확보하는 것이 우선이 되어야 합니다.
-   **`낮은 데이터 품질`** : 모델이 잘 훈련되려면 **좋은 데이터** 로 학습을 시켜야 합니다. `"Garbage in, Garbage out."` 모델에 입력하기 전에 이상치 제거, 누락값에 대한 적절한 처리 등 충분한 전처리 과정을 통해 데이터를 먼저 좋은 데이터가 되도록 손보는 과정이 꼭 필요합니다.
-   **`대표성 없는 데이터`** : 데이터는 **편향되어 있지 않아야 합니다**. 예를 들어, 각 나라별 GDP와 행복 지수에 대한 관계를 예측하려고 할 때, "부자 나라" 또는 "가난한 나라" 만의 데이터만 갖고 있다면 좋은 데이터가 아닙니다. 여러가지 특징들을 모두 잘 반영하는 대표성 있는 데이터가 있어야 모델이 편견을 갖지 않고 더 잘 일반화될 수 있는 모델로 학습될 것입니다.
-   **`무의미한 특성`** : 데이터에 **불필요한 특성은 제거**하는 게 좋습니다. 이 과정에는 도메인 지식을 활용해서 판단할 수 있습니다. 예를 들어 집값을 예측하는 경우 그 집에 살고 있는 사람의 수 같은 특징은 불필요할 수 있습니다. (이는 사람에 따라 판단 기준이 달라질 수 있습니다. 그런만큼 데이터에 대한 더 깊은 이해가 필요하고, 가능하다면 더 전문적인 지식, 정확한 지식이 있을 수록 유리할 수 있습니다.)
-   **`과소적합(underfitting)`** : 과소적합은 아직 모델이 **훈련데이터조차 완벽하게 학습하지 못했다**는 뜻입니다. 이는 모델을 조금 더 복잡하게 (모델 파라미터가 더 많도록) 만들어서 해결할 수 있습니다. 모델이 조금 더 복잡하다면 파라미터가 더 다양하게 훈련 데이터를 받아들일 수 있다는 뜻이 되기 때문입니다. 또는 더 좋은 특성을 제거하거나, 제약을 줄이는 것 등도 방법입니다.
-   **`과대적합(overfitting)`** : 과대적합은 모델이 **훈련데이터에 대해서 너무 과하게 학습했다**는 뜻입니다. 이는 훈련데이터에 과하게 적합되어서, 아직 본 적 없는 새로운 데이터에 대해서는 일반화 능력이 떨어질 수 있다는 말이 됩니다. 그 말은 즉, 훈련데이터 외의 데이터에는 슬모가 없는 모델이라는 뜻이므로 중요한 문제입니다. 과대 적합은 일단 데이터가 적은 게 가장 첫 번째 이유일 수 있습니다. 모델이 더 잘 일반화되기 위해서는 더 많은 케이스를 보고 학습해야 합니다. 따라서 훈련 데이터를 더 많이 모으거나, 모델을 조금 단순하게 만들어보는 방법이 있을 수 있습니다.

ㅤ

## 15\. 모델이 훈련데이터에서의 성능은 좋지만 새로운 샘플에서의 일반화 성능이 나쁘다면 어떤 문제가 있는 건가요? 가능한 해결책은 무엇인가요?
모델이 훈련데이터에서의 성능만 좋고, 새로운 데이터에 대해서는 예측을 잘 못한다면 이는 모델이 훈련데이터에 **과대적합(overfitting)** 되었을 가능성이 높습니다. 이에 대한 해결책은 위에서 말한 것처럼 **데이터를 더 많이 모으거나, 모델을 단순화하거나, 데이터를 더 잘 정제하는 것** 등이 있습니다.

ㅤ

## 16\. 테스트 세트가 무엇이고 왜 사용해야 하나요?
모델을 훈련시킬 때에는 데이터셋을 세 가지로 나누어 놓는 것이 바람직합니다. 세 가지 분류는 **`훈련데이터 train data`**, **`검증데이터 validation data`**, **`테스트데이터 test data`** 입니다.
- **`훈련 데이터`** 는 말 그대로 모델을 훈련시키는 데에 사용합니다. 하지만 데이터가 훈련데이터로만 구성될 경우 위에서 확인한 **과대적합** 등의 문제가 생길 수 있습니다. 또한 아직 보지 않은 (unseen) 데이터에 대한 검증이 필요합니다.
- 따라서 **`검증 데이터`** 로 특정 시기마다 모델의 학습 정도를 검증합니다. 훈련 데이터로 한 번 훈련을 한 뒤에는 검증데이터로 훈련데이터가 과대적합되지 않았는지 확인합니다. 또한 최상의 성능을 내는 모델을 결정할 때 **검증세트에서 최상의 성능을 내는 하이퍼파라미터를 선택**합니다. 훈련데이터에서 최상의 성능을 내는 하이퍼파라미터를 선택하지 않는 이유는 그 파라미터는 훈련데이터에만 최적화 되는 파라미터일 수 있기 때문입니다.
- **`테스트 데이터`** 는 모델을 론칭하기 전에 **일반화 오차(generalization error)** 를 추정하는데에 사용됩니다. 일반화 오차는 앞으로 현실에서 모델이 사용될 때 전혀 처음보는 새로운 데이터에 대해 만드는 오차입니다. 모델을 론칭하기 전에 일반화오차를 예측해 볼 필요가 있으므로 모델이 훈련 또는 검증하는 과정에서 한 번도 보지 못한 테스트 데이터를 아껴둔 후 마지막 단계에서 테스트 데이터를 통해 일반화오차라고 신뢰할 수 있는 값을 얻을 수 있습니다. 따라서 테스트 데이터는 **훈련을 마치는 마지막까지 모델에 한 번도 돌려보지 않는다**는 점이 중요합니다.

ㅤ
## 17\. 검증 세트의 목적은 무엇인가요?
검증 세트는 **모델이 훈련 데이터에 과대적합 되지 않았는지 확인**하는 데에 쓰일 수 있습니다.  
또한 **하이퍼파라미터를 결정할 때 검증세트에서 최상의 성능을 내는 파라미터로 결정**합니다.

ㅤ
## 18\. 테스트 세트를 사용해 하이퍼 파라미터를 튜닝하면 어떤 문제가 생기나요?
테스트 데이터는 모델의 **일반화오차**를 추정하는 데에 쓰이는 데이터입니다. 따라서 테스트 데이터는 **훈련 과정에서 모델이 경험하지 않게 하는 것**이 중요합니다.   
테스트 데이터를 이용해서 하이퍼 파라미터를 튜닝하면 모델은 테스트 데이터에 과대적합 될 우려가 있고, 이는 새로운 데이터에 대해 일반화 성능이 떨어지는 것으로 연결될 수 있습니다. 또한 신뢰할 수 있는 일반화 오차를 예측할 수 없습니다.
ㅤ

## 19\. 교차 검증이 무엇이고, 왜 하나의 검증 세트보다 선호하나요?
모델을 훈련할 때 데이터는 **`훈련 데이터`** / **`검증 데이터`** / **`테스트 데이터`** 세 가지로 구분합니다. 이 때 검증 데이터를 하나의 검증세트로 분리시켜 놓는다면 검증데이터를 충분히 활용할 수 없다는 단점이 있습니다. 따라서 하나의 검증세트만을 사용하기보다, 훈련데이터의 특정 비율(ex. 0.2)의 여러 subset으로 쪼개서 쓰는 **교차검증 기법**을 많이 활용합니다.   
교차검증은 여러 subset으로 나뉜 데이터 세트 중 일정 범위만큼의 데이터 세트로 모델을 훈련시킨 후 나머지 남은 데이터셋으로 검증하는 방법입니다. 교차검증 기법을 이용하면 모델을 훈련시키고 검증하는 과정을 반복함에 있어서 **데이터를 낭비하지 않고 최대한 활용**할 수 있습니다.

ㅤ
